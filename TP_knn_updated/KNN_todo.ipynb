{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $k$-nearest neighbors (KNN, $k$-NN) algorithm\n",
    "\n",
    "Authors:\n",
    "\n",
    "Joseph Salmon, Alexandre Gramfort, Claire Vernade, Mathurin Massias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy import stats\n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "from tp_knn_source import (rand_gauss, rand_bi_gauss, rand_tri_gauss,\n",
    "                           rand_checkers, rand_clown, plot_2d, ErrorCurve,\n",
    "                           frontiere_new, LOOCurve)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.close('all')\n",
    "rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 16,\n",
    "          'legend.fontsize': 16,\n",
    "          'text.usetex': False,\n",
    "          'figure.figsize': (8, 6)}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.axes_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # fix seed globally\n",
    "\n",
    "n = 100\n",
    "# infer the parameters and choose their values\n",
    "rand_gauss(n, mu, sigma)\n",
    "\n",
    "n1 = 20\n",
    "n2 = 20\n",
    "# TODO for four functions\n",
    "X1, y1 = rand_bi_gauss()\n",
    "\n",
    "n1 = 50\n",
    "n2 = 50\n",
    "n3 = 50\n",
    "X2, y2 = rand_tri_gauss()\n",
    "\n",
    "n1 = 50\n",
    "n2 = 50\n",
    "\n",
    "X3, y3 = rand_checkers\n",
    "\n",
    "n1 = 150\n",
    "n2 = 150\n",
    "X4, y4 = rand_clown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#     Displaying labeled data\n",
    "############################################################################\n",
    "\n",
    "plt.show()\n",
    "plt.close(\"all\")\n",
    "plt.ion()\n",
    "plt.figure(1, figsize=(15, 5))\n",
    "plt.subplot(141)\n",
    "plt.title('First data set')\n",
    "plot_2d(X1, y1)\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.title('Second data set')\n",
    "# todo plot,\n",
    "# todo other datasets on other subplots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $k$-NN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own implementation\n",
    "\n",
    "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Home made KNN Classifier class\"\"\"\n",
    "    def __init__(self, n_neighbors=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # no work is done at fit time, except storing training data\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO : Compute all pairwise distances between X and self.X_\n",
    "\n",
    "        # TODO : Get indices to sort them\n",
    "\n",
    "        # TODO Get indices of neighbors\n",
    "\n",
    "        # TODO: Get labels of neighbors\n",
    "        Y_neighbors = # TODO\n",
    "\n",
    "        # TODO : Find the predicted labels y for each entry in X\n",
    "        # You can use the scipy.stats.mode function\n",
    "        y_pred = # TODO\n",
    "        return y_pred\n",
    "\n",
    "# TODO : compare your implementation with scikit-learn\n",
    "\n",
    "# Focus on dataset 2\n",
    "X_train = X2[::2]\n",
    "Y_train = \n",
    "X_test = \n",
    "Y_test = \n",
    "\n",
    "# TODO\n",
    "\n",
    "# your classifier\n",
    "n_neighbors = 1\n",
    "knn = KNNClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "Y_pred = # TODO\n",
    "\n",
    "sknn = # TODO\n",
    "Y_pred_skl = # todo\n",
    "\n",
    "# TODO check that all labels match\n",
    "\n",
    "# From now on use the Scikit-Learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test now for all datasets\n",
    "\n",
    "n_neighbors = 5  # the k in k-NN\n",
    "knn = neighbors.KNeighborsClassifier # todo\n",
    "\n",
    "\n",
    "# TODO something like:\n",
    "# for data in [data1, data2, data3, data4]:\n",
    "    # TODO: fit your knn in the loop\n",
    "    \n",
    "    plt.figure()\n",
    "    #todo plot\n",
    "    n_labels = # TODO\n",
    "    frontiere_new(knn, X, y, w=None, step=50, alpha_choice=1,\n",
    "                  n_labels=n_labels, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions when varying the value of k\n",
    "\n",
    "\n",
    "plt.figure(3, figsize=(12, 8))\n",
    "plt.subplot(3, 5, 3)\n",
    "plot_2d(X_train, Y_train)\n",
    "plt.xlabel('Samples')\n",
    "ax = plt.gca()\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.get_xaxis().set_ticks([])\n",
    "\n",
    "for n_neighbors in # TODO:\n",
    "    # TODO \n",
    "    plt.subplot(3, 5, 5 + n_neighbors)\n",
    "    # todo put a label indicating the number of neighbors used in the algo\n",
    "    \n",
    "\n",
    "    frontiere_new(knn, X, y, w=None, step=50, alpha_choice=1,\n",
    "                  colorbar=False, samples=False)\n",
    "    plt.draw()  # update plot\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on train data\n",
    "n_neighbors = 1\n",
    "\n",
    "\n",
    "\n",
    "# TODO use knn.score, on test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on left out data\n",
    "\n",
    "n1 = n2 = 200\n",
    "sigma = 0.1\n",
    "data4 = rand_checkers(2 * n1, 2 * n2, sigma)\n",
    "\n",
    "X_train = X4[::2]\n",
    "Y_train = y4[::2].astype(int)\n",
    "X_test = X4[1::2]\n",
    "Y_test = y4[1::2].astype(int)\n",
    "\n",
    "\n",
    "# TODO instantiate ErrorCurve with k_range=range(1, 51)\n",
    "error_curve = \n",
    "# TODO fit it, plot it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = ['blue', 'grey', 'red', 'purple', 'orange', 'salmon', 'black',\n",
    "           'fuchsia']\n",
    "\n",
    "sigma = 0.1\n",
    "plt.figure(5)\n",
    "range_n_samples = [100, 500, 1000]\n",
    "niter = len(range_n_samples)\n",
    "for n in range(niter):\n",
    "    n1 = n2 = range_n_samples[n]\n",
    "    X_train, Y_train = rand_checkers(n1, n2, sigma)\n",
    "    X_test, Y_test = rand_checkers(n1, n2, sigma)\n",
    "    # TODO fit and plot with color varying from collist\n",
    "\n",
    "plt.legend([\"training size : %d\" % n for n in range_n_samples],\n",
    "           loc='upper left')\n",
    "\n",
    "plt.close(6)\n",
    "plt.figure(6)\n",
    "plot_2d(X_train, Y_train)\n",
    "n_neighbors = 40\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "frontiere_new(knn, X_train, Y_train, w=None, step=50, alpha_choice=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the pros and cons of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to the DIGITS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test k-NN on digits dataset\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(type(digits))\n",
    "# A Bunch is a subclass of 'dict' (dictionary)\n",
    "# help(dict)\n",
    "\n",
    "\n",
    "# inspect digits attributes:\n",
    "print(digits.keys())\n",
    "print(digits.target[:50])\n",
    "print(len(digits.data))\n",
    "print(digits.data[0])\n",
    "print(digits['data'][0])\n",
    "print(digits['images'][0])\n",
    "print(digits.data[0] == digits['data'][0])\n",
    "\n",
    "\n",
    "for idx, (img, lbl) in enumerate(list(zip(digits.images,\n",
    "                                          digits.target))[10:20]):\n",
    "    plt.subplot(2, 5, idx + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='None')\n",
    "    plt.title('Training: %i' % lbl)\n",
    "\n",
    "n_samples = len(digits.data)\n",
    "\n",
    "X_train = \n",
    "Y_train = \n",
    "X_test = =\n",
    "Y_test = \n",
    "\n",
    "plt.figure()\n",
    "# todo plot histogram of Y_test\n",
    "\n",
    "\n",
    "# TODO fit, print score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "\n",
    "conf_mat = # TODO \n",
    "print(conf_mat)\n",
    "\n",
    "# TODO normalize CM so that each row sums to 1\n",
    "conf_mat_normalized = \n",
    "plt.matshow(conf_mat_normalized)\n",
    "# use a colorbar, plt.imshow(interpolation='nearest') may be an alternative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate k with cross-validation\n",
    "\n",
    "# Have a look at the class 'LOOCurve', defined in the source file.\n",
    "# LOO stands for Leave One Out\n",
    "\n",
    "\n",
    "loo_curve = # TODO\n",
    "# TODO fit it\n",
    "# TODO print cross val scores\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "# TODO plot curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted $k$-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement weights for the kNN classifier\n",
    "\n",
    "\n",
    "def weights(dist):\n",
    "    \"\"\"Returns an array of weights, exponentially decreasing in the square\n",
    "    of the distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dist : a one-dimensional array of distances.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weight : array of the same size as dist\n",
    "    \"\"\"\n",
    "    # TODO: use weights equal to exp(- dist^2 / 0.1)\n",
    "    return # TODO\n",
    "\n",
    "\n",
    "n_neighbors = 5\n",
    "wknn = # TODO\n",
    "wknn.fit(X_train, Y_train)\n",
    "plt.figure(4)\n",
    "plot_2d(X_train, Y_train)\n",
    "\n",
    "\n",
    "frontiere_new(knn, X_train, Y_train, w=None, step=50, alpha_choice=1)\n",
    "\n",
    "\n",
    "print(wknn.predict(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
